{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP - PROCESSAMENTO DE LINGUAGEM NATURAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARA TRABALHAR COM O GOOGLE COLAB É NECESSÁRIO:\n",
    "# !apt-get update -qq\n",
    "# !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "# !wget -q https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz\n",
    "# !tar xf spark-3.1.2-bin-hadoop2.7.tgz\n",
    "# !pip install -q findspark\n",
    "\n",
    "import os\n",
    "import findspark\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"D:/spark-3.5.5-bin-hadoop3\"\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master('local[*]').getOrCreate()\n",
    "\n",
    "from pyspark.sql.types import DoubleType, StringType\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obs: para este projeto estamos trabalhando com o google colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "SPARK = spark.read.csv('/content/drive/ml/imdb-reviews-pt-br.csv', escape='\\\"', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEMATIZAÇÃO E STEMING (REMOÇÃO DE PALAVRAS DESNECESSARIAS PARA O PROCESSAMENTO COMO PRONOMES, PREPOSIÇÕES \"STOP-WORDS\", ETC)\n",
    "# PARA MELHORAR O RECURSO VISUAL UTILIZAREMOS O WORDCLOUD, DESSA MANEIRA VAMOS ENXERGAR QUAIS AS PALAVRAS MAIS UTILIZADAS EM UM TEXTO\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# INICAMOS PEGANDO APENAS UMA AMOSTRA DE 10% PARA AJUDAR O PROCESSAMENTO (collect é mt pesado)\n",
    "AMOSTRA = SPARK.select('text_pt').sample(fraction=0.1, seed=101)\n",
    "DATA = [texto['text_pt'] for texto in AMOSTRA.collect()]\n",
    "\n",
    "wordcloud = WordCloud(collocations=False, \n",
    "                      prefer_horizontal=1,\n",
    "                      width=1000,\n",
    "                      height=600).generate(str(DATA))\n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.imgshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOÇÃO DE CARACTERES ESPECIAIS\n",
    "import string\n",
    "print(string.punctuation)\n",
    "\n",
    "SPARK = SPARK.withColumn('texto_regex', f.regexp_replace('text_en', \"[\\$#,\\\"!%&'()*+,-./:;<=>?@[\\]^_`{|}~\\\\\\\\]\"), \"\")\n",
    "SPARK = SPARK.withColumn('texto_limpo', f.trim(SPARK.texto_regex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TOKENIZAÇÃO DO TEXTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UM TEXTO QUALQUER ===>  [UM][TEXTO][QUALQUER]\n",
    "# TEXTO COMPOSTO GUARDA-CHUVA ====> [TEXTO][COMPOSTO][GUARDA-CHUVA]\n",
    "from pyspark.ml.features import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(inputCol='texto_limpo', outputCol='tokens')\n",
    "SPARK_TOKENIZADO = tokenizer.transform(SPARK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "count_token = f.udf(lambda x: len(x), IntegerType())\n",
    "SPARK_TOKENIZADO.select('texto_limpo', 'tokens').withColumn('qtd_tokens', count_token(f.col('tokens')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REMOVENDO STOP-WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "# stop_A = stopwords('portuguese')\n",
    "\n",
    "from pyspark.ml.features import StopWordsRemover\n",
    "# stop_B = StopWordsRemover.loadDefaultStopWords('portuguese')\n",
    "\n",
    "remover = StopWordsRemover(inputCol='tokens', outputCol='texto_final')\n",
    "SPARK_TOKENIZADO_WORDS = remover.transform(SPARK_TOKENIZADO)\n",
    "SPARK_TOKENIZADO_WORDS.select('texto_limpo', 'tokens').withColumn('qtd_tokens', count_token(f.col('tokens')))\\\n",
    "                                                      .withColumn('qtd_tokens_limpos', count_token(f.col('texto_final')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BAG OF WORDS - VETORIZAÇÃO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importante ressaltar que a ordem na qual as palavras ficam no indice da vetorização\n",
    "# é referente ao model.vocabulary e não a ordem da string em si\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(inputCol='texto_final', outputCol='CountVec')\n",
    "model = cv.fit(SPARK_TOKENIZADO_WORDS)\n",
    "CV_FEATURES = model.transform(SPARK_TOKENIZADO_WORDS)\n",
    "\n",
    "######################\n",
    "# HASHING TF - ALTERNATIVA AO BAG OF WORDS COM PROCESSAMENTO MELHOR\n",
    "from pyspark.ml.feature import HashingTF\n",
    "\n",
    "hashing = HashingTF(inputCol='texto_final', outputCol='HashingTF')\n",
    "hashing.setNumFeatures(50) #Limitando a quantidade de palavras para executar\n",
    "\n",
    "SPARK_HASHING = hashing.transform(CV_FEATURES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF - PONDERAÇÃO DE PESOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import IDF\n",
    "\n",
    "idf = IDF(inpuCol='HashingTF', outputCol='features')\n",
    "idfModel = idf.fit(SPARK_HASHING)\n",
    "SPARK_IDF = idfModel.transform(SPARK_HASHING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRATANDO COLUNA DE SENTIMENTO STRING PARA BINARIO\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "string_indexer = StringIndexer(inputCol='sentiment', outputCol='label')\n",
    "SPARK = string_indexer.fit(SPARK).transform(SPARK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PIPELINE - UNINDO AS TRANSFORMAÇÕES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "#### UTILIZANDO A CLASSIFICAÇÃO PELA ARVORE DE DECISÃO\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "dtc = DecisionTreeClassifier(featuresCol='features', labelCol='label', maxDepth=10)\n",
    "\n",
    "tokenizer = Tokenizer(inputCol='texto_limpo', outputCol='tokens')\n",
    "remover = StopWordsRemover(inputCol='tokens', outputCol='texto_final')\n",
    "hashing = HashingTF(inputCol='texto_final', outputCol='HashingTF', numFeatures=1000)\n",
    "idf = IDF(inpuCol='HashingTF', outputCol='features')\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, hashing, idf, dtc])\n",
    "\n",
    "DF_FINAL = pipeline.fit(SPARK).transform(SPARK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TREINO E ANALISE DA ARVORE DE DECISÃO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TREINO, TESTE = SPARK.randomSplit([0.7, 0.3], seed=101)\n",
    "\n",
    "MODELO_DTC = pipeline.fit(TREINO)\n",
    "PREVISOES_DTC = MODELO_DTC.transform(TESTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AVALIANDO AS MÉTRICAS PELO MULTICLASS EVALUATOR (ACURACIA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator()\n",
    "print(\"Acurácia: %f\" % evaluator.evaluate(PREVISOES_DTC, {evaluator.metricName: 'accuracy'}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
