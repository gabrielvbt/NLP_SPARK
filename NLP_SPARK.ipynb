{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NPL - PROCESSAMENTO DE LINGUAGEM NATURAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARA TRABALHAR COM O GOOGLE COLAB É NECESSÁRIO:\n",
    "# !apt-get update -qq\n",
    "# !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "# !wget -q https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz\n",
    "# !tar xf spark-3.1.2-bin-hadoop2.7.tgz\n",
    "# !pip install -q findspark\n",
    "\n",
    "import os\n",
    "import findspark\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"D:/spark-3.5.5-bin-hadoop3\"\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master('local[*]').getOrCreate()\n",
    "\n",
    "from pyspark.sql.types import DoubleType, StringType\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obs: para este projeto estamos trabalhando com o google colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "SPARK = spark.read.csv('/content/drive/ml/imdb-reviews-pt-br.csv', escape='\\\"', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEMATIZAÇÃO E STEMING (REMOÇÃO DE PALAVRAS DESNECESSARIAS PARA O PROCESSAMENTO COMO PRONOMES, PREPOSIÇÕES \"STOP-WORDS\", ETC)\n",
    "# PARA MELHORAR O RECURSO VISUAL UTILIZAREMOS O WORDCLOUD, DESSA MANEIRA VAMOS ENXERGAR QUAIS AS PALAVRAS MAIS UTILIZADAS EM UM TEXTO\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# INICAMOS PEGANDO APENAS UMA AMOSTRA DE 10% PARA AJUDAR O PROCESSAMENTO (collect é mt pesado)\n",
    "AMOSTRA = SPARK.select('text_pt').sample(fraction=0.1, seed=101)\n",
    "DATA = [texto['text_pt'] for texto in AMOSTRA.collect()]\n",
    "\n",
    "wordcloud = WordCloud(collocations=False, \n",
    "                      prefer_horizontal=1,\n",
    "                      width=1000,\n",
    "                      height=600).generate(str(DATA))\n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.imgshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOÇÃO DE CARACTERES ESPECIAIS\n",
    "import string\n",
    "print(string.punctuation)\n",
    "\n",
    "SPARK = SPARK.withColumn('texto_regex', f.regexp_replace('text_en', \"[\\$#,\\\"!%&'()*+,-./:;<=>?@[\\]^_`{|}~\\\\\\\\]\"), \"\")\n",
    "SPARK = SPARK.withColumn('texto_limpo', f.trim(SPARK.texto_regex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZAÇÃO DO TEXTO\n",
    "# UM TEXTO QUALQUER ===>  [UM][TEXTO][QUALQUER]\n",
    "# TEXTO COMPOSTO GUARDA-CHUVA ====> [TEXTO][COMPOSTO][GUARDA-CHUVA]\n",
    "from pyspark.ml.features import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(inputCol='texto_limpo', outputCol='tokens')\n",
    "SPARK_TOKENIZADO = tokenizer.transform(SPARK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "count_token = f.udf(lambda x: len(x), IntegerType())\n",
    "SPARK_TOKENIZADO.select('texto_limpo', 'tokens').withColumn('qtd_tokens', count_token(f.col('tokens')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVENDO AS STOP-WORDS QUE APARECEM NA WORD CLOUD\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "# stop_A = stopwords('portuguese')\n",
    "\n",
    "from pyspark.ml.features import StopWordsRemover\n",
    "# stop_B = StopWordsRemover.loadDefaultStopWords('portuguese')\n",
    "\n",
    "remover = StopWordsRemover(inputCol='tokens', outputCol='texto_final')\n",
    "SPARK_TOKENIZADO_WORDS = remover.transform(SPARK_TOKENIZADO)\n",
    "SPARK_TOKENIZADO_WORDS.select('texto_limpo', 'tokens').withColumn('qtd_tokens', count_token(f.col('tokens')))\\\n",
    "                                                      .withColumn('qtd_tokens_limpos', count_token(f.col('texto_final')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
